{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_exercises.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XpCK6kwHcUso"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Construct a tf.data.Dataset\n",
        "# ds = tfds.load('mnist', split='train', shuffle_files=True)\n",
        "\n",
        "# # Build your input pipeline\n",
        "# ds = ds.shuffle(1024).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "# for example in ds.take(1):\n",
        "#   image, label = example[\"image\"], example[\"label\"]\n",
        "\n",
        "# (train_ds, test_ds), info_ds = tfds.load('wine_quality', split=['train[:75%]', 'train[75%:]'], shuffle_files=True, as_supervised=True, with_info=True)\n",
        "# tfds.as_dataframe(train_ds.take(4), info_ds)"
      ],
      "metadata": {
        "id": "dJe62nB-ciZw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create your own regression dataset (or make the one we created in \"Create data to view and fit\" bigger) and build fit a model to it."
      ],
      "metadata": {
        "id": "nA-y5LU-si9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_and_test_splits(train_size, batch_size=1):\n",
        "    # We prefetch with a buffer the same size as the dataset because th dataset\n",
        "    # is very small and fits into memory.\n",
        "    dataset = (\n",
        "        tfds.load(name=\"wine_quality\", as_supervised=True, split=\"train\")\n",
        "        .map(lambda x, y: (x, tf.cast(y, tf.float32)))\n",
        "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "        .cache()\n",
        "    )\n",
        "    # We shuffle with a buffer the same size as the dataset.\n",
        "    train_dataset = (\n",
        "        dataset.take(train_size).shuffle(buffer_size=train_size).batch(batch_size)\n",
        "    )\n",
        "    test_dataset = dataset.skip(train_size).batch(batch_size)\n",
        "\n",
        "    return train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "gUT0nGvUrM3E"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_size = 4898\n",
        "batch_size = 256\n",
        "train_size = int(dataset_size * 0.85)\n",
        "train, test = get_train_and_test_splits(train_size, batch_size)"
      ],
      "metadata": {
        "id": "ON2JJGgAriqr"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXDbUE7ErmDV",
        "outputId": "5d2361e7-5c41-46ff-8d79-1f7c0fc79981"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ({alcohol: (None,), chlorides: (None,), citric acid: (None,), density: (None,), fixed acidity: (None,), free sulfur dioxide: (None,), pH: (None,), residual sugar: (None,), sulphates: (None,), total sulfur dioxide: (None,), volatile acidity: (None,)}, (None,)), types: ({alcohol: tf.float32, chlorides: tf.float32, citric acid: tf.float32, density: tf.float32, fixed acidity: tf.float32, free sulfur dioxide: tf.float32, pH: tf.float32, residual sugar: tf.float32, sulphates: tf.float64, total sulfur dioxide: tf.float32, volatile acidity: tf.float32}, tf.float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FEATURE_NAMES = [\n",
        "    \"fixed acidity\",\n",
        "    \"volatile acidity\",\n",
        "    \"citric acid\",\n",
        "    \"residual sugar\",\n",
        "    \"chlorides\",\n",
        "    \"free sulfur dioxide\",\n",
        "    \"total sulfur dioxide\",\n",
        "    \"density\",\n",
        "    \"pH\",\n",
        "    \"sulphates\",\n",
        "    \"alcohol\",\n",
        "]\n",
        "\n",
        "\n",
        "def create_model_inputs():\n",
        "    inputs = {}\n",
        "    for feature_name in FEATURE_NAMES:\n",
        "        inputs[feature_name] = tf.keras.layers.Input(\n",
        "            name=feature_name, shape=(1,), dtype=tf.float32\n",
        "        )\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "a9EoDhNvsD0W"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = create_model_inputs()\n",
        "input_values = [value for _, value in sorted(inputs.items())]\n",
        "features = tf.keras.layers.concatenate(input_values)\n",
        "x = tf.keras.layers.Normalization()(features)\n",
        "x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
        "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
        "outputs = tf.keras.layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "1aZHYIqNcZft"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='Adam', loss='mse')"
      ],
      "metadata": {
        "id": "joahaEVctxUL"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train, epochs=10, validation_data=test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJ2DGzaViWqY",
        "outputId": "7a0430b5-faef-4ab7-a9ee-e418796d6ad1"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "17/17 [==============================] - 1s 17ms/step - loss: 61.1476 - val_loss: 23.5034\n",
            "Epoch 2/10\n",
            "17/17 [==============================] - 0s 11ms/step - loss: 10.3810 - val_loss: 6.6215\n",
            "Epoch 3/10\n",
            "17/17 [==============================] - 0s 10ms/step - loss: 4.9863 - val_loss: 3.5911\n",
            "Epoch 4/10\n",
            "17/17 [==============================] - 0s 10ms/step - loss: 3.6060 - val_loss: 2.9685\n",
            "Epoch 5/10\n",
            "17/17 [==============================] - 0s 10ms/step - loss: 2.9478 - val_loss: 2.4445\n",
            "Epoch 6/10\n",
            "17/17 [==============================] - 0s 10ms/step - loss: 2.3499 - val_loss: 1.9509\n",
            "Epoch 7/10\n",
            "17/17 [==============================] - 0s 10ms/step - loss: 1.8192 - val_loss: 1.4466\n",
            "Epoch 8/10\n",
            "17/17 [==============================] - 0s 10ms/step - loss: 1.2544 - val_loss: 0.9639\n",
            "Epoch 9/10\n",
            "17/17 [==============================] - 0s 12ms/step - loss: 0.8899 - val_loss: 0.7633\n",
            "Epoch 10/10\n",
            "17/17 [==============================] - 0s 10ms/step - loss: 0.7569 - val_loss: 0.6834\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4960f32fd0>"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try building a neural network with 4 Dense layers and fitting it to your own regression dataset, how does it perform?\n"
      ],
      "metadata": {
        "id": "iHr-NHpj-PEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Try building a neural network with 4 Dense layers and fitting it to your own regression dataset, how does it perform?\n",
        "\n",
        "z = tf.keras.layers.Normalization()(features)\n",
        "for i in range(4):\n",
        "  z = tf.keras.layers.Dense(64)(z)\n",
        "outputs = tf.keras.layers.Dense(1, activation='linear')(z)\n",
        "\n",
        "model2 = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "model2.compile(optimizer='Adam', loss='mse')"
      ],
      "metadata": {
        "id": "SCzXC0smcqA3"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model2.summary()"
      ],
      "metadata": {
        "id": "b1SEl-lLuVEP"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.fit(train, epochs=10, validation_data=test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6nYtKWXuaSw",
        "outputId": "9dd620ef-0215-4492-cb4f-03c719440fef"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "17/17 [==============================] - 1s 18ms/step - loss: 184.1071 - val_loss: 14.6813\n",
            "Epoch 2/10\n",
            "17/17 [==============================] - 0s 10ms/step - loss: 19.7233 - val_loss: 7.0935\n",
            "Epoch 3/10\n",
            "17/17 [==============================] - 0s 11ms/step - loss: 5.9101 - val_loss: 3.9109\n",
            "Epoch 4/10\n",
            "17/17 [==============================] - 0s 10ms/step - loss: 2.6820 - val_loss: 1.7774\n",
            "Epoch 5/10\n",
            "17/17 [==============================] - 0s 29ms/step - loss: 1.6383 - val_loss: 1.1237\n",
            "Epoch 6/10\n",
            "17/17 [==============================] - 0s 11ms/step - loss: 1.0439 - val_loss: 0.7969\n",
            "Epoch 7/10\n",
            "17/17 [==============================] - 0s 10ms/step - loss: 0.7899 - val_loss: 0.6812\n",
            "Epoch 8/10\n",
            "17/17 [==============================] - 0s 12ms/step - loss: 0.7060 - val_loss: 0.6304\n",
            "Epoch 9/10\n",
            "17/17 [==============================] - 0s 10ms/step - loss: 0.6648 - val_loss: 0.6230\n",
            "Epoch 10/10\n",
            "17/17 [==============================] - 0s 10ms/step - loss: 0.6691 - val_loss: 0.6335\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f49599455d0>"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try and improve the results we got on the insurance dataset, some things you might want to try include:\n"
      ],
      "metadata": {
        "id": "qAkJGPULupAB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a larger model (how does one with 4 dense layers go?).\n"
      ],
      "metadata": {
        "id": "CHTu2Mnsury-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read in the insurance dataset\n",
        "insurance = pd.read_csv(\"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\", low_memory=False)\n",
        "\n",
        "# Check out the insurance dataset\n",
        "insurance.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "jbcO4zAivBTm",
        "outputId": "0d2e15f1-4243-4fcc-f606-d690b18a3b27"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8e7ceecf-1af1-4e5b-8ca4-5416f5c508de\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>bmi</th>\n",
              "      <th>children</th>\n",
              "      <th>smoker</th>\n",
              "      <th>region</th>\n",
              "      <th>charges</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>19</td>\n",
              "      <td>female</td>\n",
              "      <td>27.900</td>\n",
              "      <td>0</td>\n",
              "      <td>yes</td>\n",
              "      <td>southwest</td>\n",
              "      <td>16884.92400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18</td>\n",
              "      <td>male</td>\n",
              "      <td>33.770</td>\n",
              "      <td>1</td>\n",
              "      <td>no</td>\n",
              "      <td>southeast</td>\n",
              "      <td>1725.55230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>28</td>\n",
              "      <td>male</td>\n",
              "      <td>33.000</td>\n",
              "      <td>3</td>\n",
              "      <td>no</td>\n",
              "      <td>southeast</td>\n",
              "      <td>4449.46200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33</td>\n",
              "      <td>male</td>\n",
              "      <td>22.705</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>northwest</td>\n",
              "      <td>21984.47061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>32</td>\n",
              "      <td>male</td>\n",
              "      <td>28.880</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>northwest</td>\n",
              "      <td>3866.85520</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8e7ceecf-1af1-4e5b-8ca4-5416f5c508de')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8e7ceecf-1af1-4e5b-8ca4-5416f5c508de button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8e7ceecf-1af1-4e5b-8ca4-5416f5c508de');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   age     sex     bmi  children smoker     region      charges\n",
              "0   19  female  27.900         0    yes  southwest  16884.92400\n",
              "1   18    male  33.770         1     no  southeast   1725.55230\n",
              "2   28    male  33.000         3     no  southeast   4449.46200\n",
              "3   33    male  22.705         0     no  northwest  21984.47061\n",
              "4   32    male  28.880         0     no  northwest   3866.85520"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subset = ['sex', 'smoker', 'region']\n",
        "dumIns = pd.get_dummies(insurance, columns=subset)\n",
        "dumIns.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "aPTOg9Fpvr41",
        "outputId": "7b96caa0-81bf-4790-c35e-82ea1baa44bd"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-73584fc9-e480-4297-a734-d2b4ed352434\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>bmi</th>\n",
              "      <th>children</th>\n",
              "      <th>charges</th>\n",
              "      <th>sex_female</th>\n",
              "      <th>sex_male</th>\n",
              "      <th>smoker_no</th>\n",
              "      <th>smoker_yes</th>\n",
              "      <th>region_northeast</th>\n",
              "      <th>region_northwest</th>\n",
              "      <th>region_southeast</th>\n",
              "      <th>region_southwest</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>19</td>\n",
              "      <td>27.900</td>\n",
              "      <td>0</td>\n",
              "      <td>16884.92400</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18</td>\n",
              "      <td>33.770</td>\n",
              "      <td>1</td>\n",
              "      <td>1725.55230</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>28</td>\n",
              "      <td>33.000</td>\n",
              "      <td>3</td>\n",
              "      <td>4449.46200</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33</td>\n",
              "      <td>22.705</td>\n",
              "      <td>0</td>\n",
              "      <td>21984.47061</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>32</td>\n",
              "      <td>28.880</td>\n",
              "      <td>0</td>\n",
              "      <td>3866.85520</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-73584fc9-e480-4297-a734-d2b4ed352434')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-73584fc9-e480-4297-a734-d2b4ed352434 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-73584fc9-e480-4297-a734-d2b4ed352434');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   age     bmi  children  ...  region_northwest  region_southeast  region_southwest\n",
              "0   19  27.900         0  ...                 0                 0                 1\n",
              "1   18  33.770         1  ...                 0                 1                 0\n",
              "2   28  33.000         3  ...                 0                 1                 0\n",
              "3   33  22.705         0  ...                 1                 0                 0\n",
              "4   32  28.880         0  ...                 1                 0                 0\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = dumIns.drop('charges', axis=1)\n",
        "y = dumIns.charges"
      ],
      "metadata": {
        "id": "D__RVUFCwr1l"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X.sample(len(X))\n",
        "idx = int(.75*len(X))\n",
        "X_train, X_test = X[:idx], X[idx:]"
      ],
      "metadata": {
        "id": "3F8khZVUw-nD"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train, y_test = y.loc[X_train.index], y.loc[X_test.index]"
      ],
      "metadata": {
        "id": "gUuOFZkCxWdu"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.data.Dataset.from_tensor_slices(X_train) # Tempting to do this, but fit skips the necessity with pd df\n",
        "# more indepth: https://colab.research.google.com/github/adammichaelwood/tf-docs/blob/csv-feature-columns/site/en/r2/tutorials/load_data/csv.ipynb#scrollTo=kPWkC4_1l3IG"
      ],
      "metadata": {
        "id": "Rl6sc1xwxmYQ"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42) \n",
        "\n",
        "ins1 = tf.keras.Sequential()\n",
        "ins1.add(tf.keras.layers.Input([X_train.shape[1],]))\n",
        "ins1.add(tf.keras.layers.Normalization())\n",
        "ins1.add(tf.keras.layers.Dense(100, activation='relu'))\n",
        "ins1.add(tf.keras.layers.Dense(100, activation='relu'))\n",
        "ins1.add(tf.keras.layers.Dense(100, activation='relu'))\n",
        "ins1.add(tf.keras.layers.Dense(100, activation='relu'))\n",
        "ins1.add(tf.keras.layers.Dense(1, activation='linear'))\n",
        "\n",
        "ins1.compile(optimizer='Adam', loss='mae')"
      ],
      "metadata": {
        "id": "wy0VxjaOwBbg"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = ins1.fit(X_train, y_train, validation_data=[X_test, y_test], epochs=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOing7cVy9qY",
        "outputId": "f3eed622-f260-4722-957f-09b807e840c7"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "32/32 [==============================] - 2s 21ms/step - loss: 12967.7793 - val_loss: 13783.4639\n",
            "Epoch 2/200\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 11405.4111 - val_loss: 9206.5869\n",
            "Epoch 3/200\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 7459.7207 - val_loss: 8163.5244\n",
            "Epoch 4/200\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 7096.9370 - val_loss: 7930.1538\n",
            "Epoch 5/200\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 6968.8101 - val_loss: 7837.1133\n",
            "Epoch 6/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 6851.7329 - val_loss: 7725.5566\n",
            "Epoch 7/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 6725.3086 - val_loss: 7605.5972\n",
            "Epoch 8/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 6598.2803 - val_loss: 7489.8433\n",
            "Epoch 9/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 6450.8267 - val_loss: 7405.7637\n",
            "Epoch 10/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 6357.3706 - val_loss: 7358.1680\n",
            "Epoch 11/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 6303.4629 - val_loss: 7318.6709\n",
            "Epoch 12/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 6261.5698 - val_loss: 7276.9106\n",
            "Epoch 13/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 6216.9722 - val_loss: 7234.8442\n",
            "Epoch 14/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 6178.9067 - val_loss: 7261.6284\n",
            "Epoch 15/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 6127.8540 - val_loss: 7138.2827\n",
            "Epoch 16/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 6056.6934 - val_loss: 7075.4170\n",
            "Epoch 17/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 5994.2383 - val_loss: 7004.5620\n",
            "Epoch 18/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 5907.8755 - val_loss: 7007.0591\n",
            "Epoch 19/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5851.6338 - val_loss: 6821.4346\n",
            "Epoch 20/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5693.6782 - val_loss: 6660.7207\n",
            "Epoch 21/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 5540.2847 - val_loss: 6490.9155\n",
            "Epoch 22/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 5353.5132 - val_loss: 6281.1938\n",
            "Epoch 23/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5088.9595 - val_loss: 5914.7842\n",
            "Epoch 24/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4696.4287 - val_loss: 5452.8301\n",
            "Epoch 25/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4221.1704 - val_loss: 4803.1636\n",
            "Epoch 26/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 3719.3435 - val_loss: 4369.9922\n",
            "Epoch 27/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 3501.1689 - val_loss: 4304.3418\n",
            "Epoch 28/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 3442.1855 - val_loss: 4253.8481\n",
            "Epoch 29/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 3442.7300 - val_loss: 4149.0801\n",
            "Epoch 30/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 3372.0037 - val_loss: 4123.3486\n",
            "Epoch 31/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 3340.4397 - val_loss: 4234.9468\n",
            "Epoch 32/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 3345.5852 - val_loss: 4246.1797\n",
            "Epoch 33/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 3349.4612 - val_loss: 4117.5864\n",
            "Epoch 34/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 3319.7502 - val_loss: 4043.9880\n",
            "Epoch 35/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 3256.0166 - val_loss: 3969.3818\n",
            "Epoch 36/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 3252.1204 - val_loss: 3959.8066\n",
            "Epoch 37/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 3217.5146 - val_loss: 3900.5129\n",
            "Epoch 38/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 3199.0203 - val_loss: 3809.8250\n",
            "Epoch 39/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 3148.6067 - val_loss: 3768.1008\n",
            "Epoch 40/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 3172.1096 - val_loss: 4060.3716\n",
            "Epoch 41/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 3162.0427 - val_loss: 3602.1970\n",
            "Epoch 42/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 3056.9302 - val_loss: 3651.7742\n",
            "Epoch 43/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 3016.9995 - val_loss: 3485.8806\n",
            "Epoch 44/200\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 3057.1643 - val_loss: 3459.0698\n",
            "Epoch 45/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2999.9661 - val_loss: 3352.0154\n",
            "Epoch 46/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2910.2554 - val_loss: 3226.7727\n",
            "Epoch 47/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2878.4199 - val_loss: 3218.6270\n",
            "Epoch 48/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2827.9397 - val_loss: 3102.7053\n",
            "Epoch 49/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2809.7136 - val_loss: 3037.0134\n",
            "Epoch 50/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2761.1101 - val_loss: 3129.6697\n",
            "Epoch 51/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2730.5845 - val_loss: 2990.6394\n",
            "Epoch 52/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2709.2483 - val_loss: 3132.0432\n",
            "Epoch 53/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2736.3232 - val_loss: 3029.5984\n",
            "Epoch 54/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2694.8816 - val_loss: 3025.4189\n",
            "Epoch 55/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2651.9148 - val_loss: 2978.8191\n",
            "Epoch 56/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2644.7932 - val_loss: 2940.2759\n",
            "Epoch 57/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2652.8879 - val_loss: 2924.4431\n",
            "Epoch 58/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2595.6079 - val_loss: 2963.3394\n",
            "Epoch 59/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2636.8667 - val_loss: 2860.3137\n",
            "Epoch 60/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2634.8916 - val_loss: 3023.5298\n",
            "Epoch 61/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2637.5618 - val_loss: 2913.9182\n",
            "Epoch 62/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2565.7434 - val_loss: 2912.0518\n",
            "Epoch 63/200\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 2554.8738 - val_loss: 2872.0898\n",
            "Epoch 64/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2533.0820 - val_loss: 2973.9463\n",
            "Epoch 65/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2549.8152 - val_loss: 2760.4402\n",
            "Epoch 66/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2511.1829 - val_loss: 2956.0881\n",
            "Epoch 67/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2579.0833 - val_loss: 2939.6863\n",
            "Epoch 68/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2538.3569 - val_loss: 2958.1621\n",
            "Epoch 69/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2512.2722 - val_loss: 2880.0740\n",
            "Epoch 70/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2489.0659 - val_loss: 2769.1677\n",
            "Epoch 71/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2454.6099 - val_loss: 2779.4814\n",
            "Epoch 72/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2441.4487 - val_loss: 2696.3110\n",
            "Epoch 73/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2413.8779 - val_loss: 2638.3777\n",
            "Epoch 74/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2406.5630 - val_loss: 2705.6223\n",
            "Epoch 75/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2417.9841 - val_loss: 2642.9109\n",
            "Epoch 76/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2407.4180 - val_loss: 2861.4172\n",
            "Epoch 77/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2412.1318 - val_loss: 2835.0647\n",
            "Epoch 78/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2393.3784 - val_loss: 2561.6438\n",
            "Epoch 79/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2395.7271 - val_loss: 2634.9917\n",
            "Epoch 80/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2345.6365 - val_loss: 2598.0940\n",
            "Epoch 81/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2364.6108 - val_loss: 2546.6733\n",
            "Epoch 82/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2456.1150 - val_loss: 3005.0037\n",
            "Epoch 83/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2424.2705 - val_loss: 2526.5215\n",
            "Epoch 84/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2305.9419 - val_loss: 2587.5969\n",
            "Epoch 85/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2262.6013 - val_loss: 2509.4646\n",
            "Epoch 86/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2297.1799 - val_loss: 2601.0508\n",
            "Epoch 87/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2277.8323 - val_loss: 2644.2615\n",
            "Epoch 88/200\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 2274.2761 - val_loss: 2492.7180\n",
            "Epoch 89/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2245.9080 - val_loss: 2548.0796\n",
            "Epoch 90/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2233.1055 - val_loss: 2572.1711\n",
            "Epoch 91/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2229.4006 - val_loss: 2469.2729\n",
            "Epoch 92/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2224.2522 - val_loss: 2507.8079\n",
            "Epoch 93/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2195.0166 - val_loss: 2500.1704\n",
            "Epoch 94/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2159.6951 - val_loss: 2449.0867\n",
            "Epoch 95/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2157.4534 - val_loss: 2438.8181\n",
            "Epoch 96/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2189.6873 - val_loss: 2570.3159\n",
            "Epoch 97/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2218.0024 - val_loss: 2532.6394\n",
            "Epoch 98/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2217.8193 - val_loss: 2608.3501\n",
            "Epoch 99/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2201.2122 - val_loss: 2514.9583\n",
            "Epoch 100/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2132.0525 - val_loss: 2420.9358\n",
            "Epoch 101/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2165.8655 - val_loss: 2506.1526\n",
            "Epoch 102/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2138.1462 - val_loss: 2569.3308\n",
            "Epoch 103/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2153.0210 - val_loss: 2440.1130\n",
            "Epoch 104/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2125.4358 - val_loss: 2468.1951\n",
            "Epoch 105/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2125.9927 - val_loss: 2526.4028\n",
            "Epoch 106/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2176.4409 - val_loss: 2528.3264\n",
            "Epoch 107/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2124.6509 - val_loss: 2448.4900\n",
            "Epoch 108/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2073.8760 - val_loss: 2451.8391\n",
            "Epoch 109/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2071.3474 - val_loss: 2413.9001\n",
            "Epoch 110/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2047.2231 - val_loss: 2432.6311\n",
            "Epoch 111/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2073.8035 - val_loss: 2434.3843\n",
            "Epoch 112/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2143.8691 - val_loss: 2507.3101\n",
            "Epoch 113/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2158.1506 - val_loss: 2593.4348\n",
            "Epoch 114/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2103.0459 - val_loss: 2425.3726\n",
            "Epoch 115/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2036.4346 - val_loss: 2407.7004\n",
            "Epoch 116/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2034.7407 - val_loss: 2425.9277\n",
            "Epoch 117/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2061.6472 - val_loss: 2375.3303\n",
            "Epoch 118/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2052.4382 - val_loss: 2472.5525\n",
            "Epoch 119/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2045.9199 - val_loss: 2402.7205\n",
            "Epoch 120/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2020.0054 - val_loss: 2367.8264\n",
            "Epoch 121/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2076.0337 - val_loss: 2416.2319\n",
            "Epoch 122/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2021.8203 - val_loss: 2402.5325\n",
            "Epoch 123/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1993.8514 - val_loss: 2378.2935\n",
            "Epoch 124/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1981.4100 - val_loss: 2478.8279\n",
            "Epoch 125/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2005.5441 - val_loss: 2460.7722\n",
            "Epoch 126/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2046.9291 - val_loss: 2372.7827\n",
            "Epoch 127/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1996.5674 - val_loss: 2445.2087\n",
            "Epoch 128/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2047.0880 - val_loss: 2449.2339\n",
            "Epoch 129/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2027.2578 - val_loss: 2417.3953\n",
            "Epoch 130/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1959.9805 - val_loss: 2465.0881\n",
            "Epoch 131/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1950.5499 - val_loss: 2374.2581\n",
            "Epoch 132/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1966.0255 - val_loss: 2426.5620\n",
            "Epoch 133/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1971.9840 - val_loss: 2364.7649\n",
            "Epoch 134/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1975.6387 - val_loss: 2405.2954\n",
            "Epoch 135/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1972.0056 - val_loss: 2438.2170\n",
            "Epoch 136/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1945.6499 - val_loss: 2363.7627\n",
            "Epoch 137/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1966.4696 - val_loss: 2462.3391\n",
            "Epoch 138/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1956.1714 - val_loss: 2363.7698\n",
            "Epoch 139/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1955.1726 - val_loss: 2434.0166\n",
            "Epoch 140/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1981.7357 - val_loss: 2388.9893\n",
            "Epoch 141/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1922.2852 - val_loss: 2386.9368\n",
            "Epoch 142/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1915.2632 - val_loss: 2353.9294\n",
            "Epoch 143/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1908.2290 - val_loss: 2359.1785\n",
            "Epoch 144/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1943.5255 - val_loss: 2407.4570\n",
            "Epoch 145/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2010.7238 - val_loss: 2628.3860\n",
            "Epoch 146/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1938.7673 - val_loss: 2337.7568\n",
            "Epoch 147/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1891.4622 - val_loss: 2383.6162\n",
            "Epoch 148/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1878.7045 - val_loss: 2352.1250\n",
            "Epoch 149/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1894.9641 - val_loss: 2386.8862\n",
            "Epoch 150/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1918.7380 - val_loss: 2344.9614\n",
            "Epoch 151/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1877.2483 - val_loss: 2319.1248\n",
            "Epoch 152/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1895.6558 - val_loss: 2401.6453\n",
            "Epoch 153/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1914.4706 - val_loss: 2316.9456\n",
            "Epoch 154/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1930.7948 - val_loss: 2377.7227\n",
            "Epoch 155/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1895.0707 - val_loss: 2375.0742\n",
            "Epoch 156/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1906.1487 - val_loss: 2336.7715\n",
            "Epoch 157/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1930.6097 - val_loss: 2465.8777\n",
            "Epoch 158/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1924.6937 - val_loss: 2332.2759\n",
            "Epoch 159/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1904.0663 - val_loss: 2364.5115\n",
            "Epoch 160/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1886.5422 - val_loss: 2339.3691\n",
            "Epoch 161/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1876.8735 - val_loss: 2330.0515\n",
            "Epoch 162/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1897.9116 - val_loss: 2449.1860\n",
            "Epoch 163/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1935.7701 - val_loss: 2543.4487\n",
            "Epoch 164/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1924.4741 - val_loss: 2373.2861\n",
            "Epoch 165/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1915.4827 - val_loss: 2328.1760\n",
            "Epoch 166/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1885.8882 - val_loss: 2376.7561\n",
            "Epoch 167/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1899.7168 - val_loss: 2345.9326\n",
            "Epoch 168/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1919.6804 - val_loss: 2392.3809\n",
            "Epoch 169/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1912.9192 - val_loss: 2316.9988\n",
            "Epoch 170/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1856.5127 - val_loss: 2334.8826\n",
            "Epoch 171/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1859.4658 - val_loss: 2420.6436\n",
            "Epoch 172/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1865.9022 - val_loss: 2346.9788\n",
            "Epoch 173/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1847.3737 - val_loss: 2325.3921\n",
            "Epoch 174/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1841.9104 - val_loss: 2328.0874\n",
            "Epoch 175/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1893.9956 - val_loss: 2359.7012\n",
            "Epoch 176/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1923.1978 - val_loss: 2367.0579\n",
            "Epoch 177/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1915.1971 - val_loss: 2484.0352\n",
            "Epoch 178/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1978.1978 - val_loss: 2420.7051\n",
            "Epoch 179/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1879.6447 - val_loss: 2343.6936\n",
            "Epoch 180/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1850.3157 - val_loss: 2394.4055\n",
            "Epoch 181/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1881.5334 - val_loss: 2377.6414\n",
            "Epoch 182/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1857.5374 - val_loss: 2331.6497\n",
            "Epoch 183/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1869.7253 - val_loss: 2460.5120\n",
            "Epoch 184/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1842.2606 - val_loss: 2334.9060\n",
            "Epoch 185/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1884.0698 - val_loss: 2366.8535\n",
            "Epoch 186/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1858.2343 - val_loss: 2371.8447\n",
            "Epoch 187/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1873.0894 - val_loss: 2306.5955\n",
            "Epoch 188/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1842.5159 - val_loss: 2358.0439\n",
            "Epoch 189/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1888.1525 - val_loss: 2327.3274\n",
            "Epoch 190/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1883.0454 - val_loss: 2360.0378\n",
            "Epoch 191/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1876.9331 - val_loss: 2400.7773\n",
            "Epoch 192/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1890.0721 - val_loss: 2389.0730\n",
            "Epoch 193/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1874.0430 - val_loss: 2413.4915\n",
            "Epoch 194/200\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 1922.4668 - val_loss: 2428.5376\n",
            "Epoch 195/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1862.8253 - val_loss: 2306.0693\n",
            "Epoch 196/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1819.2521 - val_loss: 2357.3521\n",
            "Epoch 197/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1823.2341 - val_loss: 2344.5422\n",
            "Epoch 198/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1857.4517 - val_loss: 2354.8921\n",
            "Epoch 199/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1870.0338 - val_loss: 2537.1484\n",
            "Epoch 200/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1950.0492 - val_loss: 2378.0422\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Increasing the number of units in each layer.\n"
      ],
      "metadata": {
        "id": "ORocUlSFzX6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "ins2 = tf.keras.Sequential()\n",
        "ins2.add(tf.keras.layers.Input([X_train.shape[1],]))\n",
        "ins2.add(tf.keras.layers.Normalization())\n",
        "ins2.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "ins2.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "ins2.add(tf.keras.layers.Dense(1))\n"
      ],
      "metadata": {
        "id": "ZPkyTY4BzbWq"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ins2.compile(optimizer='Adam', loss='mae')\n",
        "ins2.fit(X_train, y_train, validation_data=[X_test, y_test], epochs=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hclrs8gxct27",
        "outputId": "a9ea09fb-6904-4462-8b63-206c17172998"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "32/32 [==============================] - 1s 10ms/step - loss: 12940.6348 - val_loss: 13797.7197\n",
            "Epoch 2/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 12427.9893 - val_loss: 12772.8359\n",
            "Epoch 3/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 10624.3330 - val_loss: 10057.0137\n",
            "Epoch 4/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 7873.1172 - val_loss: 8141.3726\n",
            "Epoch 5/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 7242.3535 - val_loss: 8086.2778\n",
            "Epoch 6/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 7187.7705 - val_loss: 8037.4897\n",
            "Epoch 7/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 7129.9531 - val_loss: 7989.4771\n",
            "Epoch 8/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 7079.0254 - val_loss: 7938.7954\n",
            "Epoch 9/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 7019.3477 - val_loss: 7894.5283\n",
            "Epoch 10/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 6955.7749 - val_loss: 7829.9536\n",
            "Epoch 11/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 6893.1338 - val_loss: 7774.3208\n",
            "Epoch 12/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 6823.1685 - val_loss: 7722.1401\n",
            "Epoch 13/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 6762.5186 - val_loss: 7655.6162\n",
            "Epoch 14/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 6702.7031 - val_loss: 7614.4619\n",
            "Epoch 15/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 6612.3984 - val_loss: 7531.3828\n",
            "Epoch 16/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 6540.4814 - val_loss: 7458.5420\n",
            "Epoch 17/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 6461.2471 - val_loss: 7399.0908\n",
            "Epoch 18/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 6386.7627 - val_loss: 7337.9111\n",
            "Epoch 19/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 6320.9473 - val_loss: 7291.4507\n",
            "Epoch 20/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 6267.3228 - val_loss: 7255.0933\n",
            "Epoch 21/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 6219.2632 - val_loss: 7228.4634\n",
            "Epoch 22/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 6201.6016 - val_loss: 7195.6611\n",
            "Epoch 23/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 6172.9868 - val_loss: 7169.4121\n",
            "Epoch 24/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 6141.5059 - val_loss: 7144.5605\n",
            "Epoch 25/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 6104.4185 - val_loss: 7119.3013\n",
            "Epoch 26/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 6079.0894 - val_loss: 7092.4707\n",
            "Epoch 27/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 6046.9595 - val_loss: 7067.1245\n",
            "Epoch 28/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 6013.4937 - val_loss: 7036.1963\n",
            "Epoch 29/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 5981.0566 - val_loss: 7009.6450\n",
            "Epoch 30/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5952.1304 - val_loss: 7003.2759\n",
            "Epoch 31/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5913.2524 - val_loss: 6952.6733\n",
            "Epoch 32/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 5885.8589 - val_loss: 6898.1514\n",
            "Epoch 33/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5839.5811 - val_loss: 6865.5059\n",
            "Epoch 34/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5799.6343 - val_loss: 6846.5483\n",
            "Epoch 35/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5752.2222 - val_loss: 6773.9531\n",
            "Epoch 36/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 5705.1758 - val_loss: 6732.3535\n",
            "Epoch 37/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 5651.1401 - val_loss: 6665.9551\n",
            "Epoch 38/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 5591.6333 - val_loss: 6611.6465\n",
            "Epoch 39/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 5534.1416 - val_loss: 6548.6338\n",
            "Epoch 40/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 5472.1641 - val_loss: 6497.5337\n",
            "Epoch 41/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 5409.6558 - val_loss: 6448.4941\n",
            "Epoch 42/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 5329.7397 - val_loss: 6362.7700\n",
            "Epoch 43/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 5243.5752 - val_loss: 6221.7954\n",
            "Epoch 44/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 5150.7539 - val_loss: 6161.7515\n",
            "Epoch 45/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 5050.2539 - val_loss: 6062.3306\n",
            "Epoch 46/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4969.4922 - val_loss: 5925.5718\n",
            "Epoch 47/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 4843.9355 - val_loss: 5854.5566\n",
            "Epoch 48/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 4701.9141 - val_loss: 5630.3193\n",
            "Epoch 49/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 4557.0991 - val_loss: 5453.8662\n",
            "Epoch 50/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 4403.1650 - val_loss: 5274.1919\n",
            "Epoch 51/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 4233.1631 - val_loss: 5170.4849\n",
            "Epoch 52/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 4051.2341 - val_loss: 4891.2437\n",
            "Epoch 53/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 3862.4880 - val_loss: 4725.7935\n",
            "Epoch 54/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 3739.5017 - val_loss: 4532.5151\n",
            "Epoch 55/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 3574.3916 - val_loss: 4397.9414\n",
            "Epoch 56/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 3485.6982 - val_loss: 4310.2363\n",
            "Epoch 57/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 3419.3677 - val_loss: 4253.2480\n",
            "Epoch 58/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 3381.5110 - val_loss: 4205.8911\n",
            "Epoch 59/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 3350.1006 - val_loss: 4153.5054\n",
            "Epoch 60/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 3319.6167 - val_loss: 4155.2700\n",
            "Epoch 61/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 3310.0391 - val_loss: 4076.0735\n",
            "Epoch 62/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 3276.8796 - val_loss: 4061.0984\n",
            "Epoch 63/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 3262.3704 - val_loss: 4030.8010\n",
            "Epoch 64/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 3245.6702 - val_loss: 4040.6033\n",
            "Epoch 65/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 3228.8899 - val_loss: 4009.1362\n",
            "Epoch 66/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 3227.0173 - val_loss: 3999.2317\n",
            "Epoch 67/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 3206.2427 - val_loss: 3977.9102\n",
            "Epoch 68/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 3190.5349 - val_loss: 3985.5698\n",
            "Epoch 69/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 3182.9622 - val_loss: 3965.7441\n",
            "Epoch 70/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 3173.8589 - val_loss: 3939.5383\n",
            "Epoch 71/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 3171.0757 - val_loss: 3941.0120\n",
            "Epoch 72/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 3162.9019 - val_loss: 3902.9500\n",
            "Epoch 73/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 3145.8936 - val_loss: 3921.1667\n",
            "Epoch 74/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 3140.1909 - val_loss: 3888.0732\n",
            "Epoch 75/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 3141.0046 - val_loss: 3883.6414\n",
            "Epoch 76/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 3119.9417 - val_loss: 3840.5518\n",
            "Epoch 77/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 3112.8018 - val_loss: 3840.7676\n",
            "Epoch 78/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 3109.5496 - val_loss: 3822.3997\n",
            "Epoch 79/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 3101.3552 - val_loss: 3814.8455\n",
            "Epoch 80/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 3081.7505 - val_loss: 3808.7280\n",
            "Epoch 81/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 3072.7888 - val_loss: 3768.6040\n",
            "Epoch 82/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 3076.2236 - val_loss: 3768.8286\n",
            "Epoch 83/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 3056.4109 - val_loss: 3755.5142\n",
            "Epoch 84/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 3049.0029 - val_loss: 3717.7351\n",
            "Epoch 85/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 3037.7410 - val_loss: 3708.2344\n",
            "Epoch 86/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 3034.8140 - val_loss: 3743.6094\n",
            "Epoch 87/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 3024.2693 - val_loss: 3667.2959\n",
            "Epoch 88/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 3005.2939 - val_loss: 3649.3765\n",
            "Epoch 89/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2988.7173 - val_loss: 3656.6626\n",
            "Epoch 90/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 3003.6482 - val_loss: 3628.2837\n",
            "Epoch 91/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2981.2827 - val_loss: 3583.1433\n",
            "Epoch 92/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2959.9319 - val_loss: 3567.4492\n",
            "Epoch 93/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2946.5513 - val_loss: 3547.8743\n",
            "Epoch 94/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2963.2842 - val_loss: 3556.1257\n",
            "Epoch 95/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2932.6150 - val_loss: 3519.1658\n",
            "Epoch 96/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2918.5681 - val_loss: 3502.7925\n",
            "Epoch 97/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2925.3755 - val_loss: 3465.8743\n",
            "Epoch 98/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2896.2744 - val_loss: 3427.2246\n",
            "Epoch 99/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2884.8521 - val_loss: 3406.8745\n",
            "Epoch 100/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2871.2078 - val_loss: 3433.8918\n",
            "Epoch 101/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2897.6047 - val_loss: 3382.9048\n",
            "Epoch 102/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2852.2532 - val_loss: 3345.6033\n",
            "Epoch 103/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2841.7097 - val_loss: 3299.2544\n",
            "Epoch 104/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2820.0273 - val_loss: 3284.2046\n",
            "Epoch 105/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2803.1750 - val_loss: 3286.3145\n",
            "Epoch 106/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2809.9500 - val_loss: 3222.6511\n",
            "Epoch 107/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2771.2935 - val_loss: 3181.1787\n",
            "Epoch 108/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2781.2212 - val_loss: 3153.5466\n",
            "Epoch 109/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2745.3481 - val_loss: 3140.4609\n",
            "Epoch 110/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2739.6638 - val_loss: 3135.8074\n",
            "Epoch 111/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2724.2275 - val_loss: 3069.3477\n",
            "Epoch 112/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2712.6279 - val_loss: 3100.2827\n",
            "Epoch 113/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2682.8645 - val_loss: 3013.6631\n",
            "Epoch 114/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2662.1860 - val_loss: 2976.9343\n",
            "Epoch 115/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2653.1506 - val_loss: 2935.6917\n",
            "Epoch 116/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2635.7830 - val_loss: 2918.4939\n",
            "Epoch 117/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2639.4600 - val_loss: 2916.2458\n",
            "Epoch 118/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2623.2148 - val_loss: 2891.5154\n",
            "Epoch 119/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2624.1077 - val_loss: 2896.0496\n",
            "Epoch 120/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2622.0176 - val_loss: 2878.5684\n",
            "Epoch 121/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2594.2734 - val_loss: 2904.0977\n",
            "Epoch 122/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2625.9707 - val_loss: 2887.3650\n",
            "Epoch 123/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2585.7139 - val_loss: 2924.1025\n",
            "Epoch 124/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2595.7886 - val_loss: 2896.6343\n",
            "Epoch 125/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2574.6436 - val_loss: 2894.2490\n",
            "Epoch 126/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2585.9065 - val_loss: 2882.8066\n",
            "Epoch 127/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2578.1621 - val_loss: 2895.9019\n",
            "Epoch 128/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2557.5625 - val_loss: 2854.1589\n",
            "Epoch 129/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2547.6702 - val_loss: 2856.9170\n",
            "Epoch 130/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2573.8181 - val_loss: 2879.9880\n",
            "Epoch 131/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2542.1323 - val_loss: 2804.8931\n",
            "Epoch 132/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2532.6570 - val_loss: 2868.7036\n",
            "Epoch 133/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2527.6646 - val_loss: 2816.0854\n",
            "Epoch 134/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2522.1155 - val_loss: 2859.5422\n",
            "Epoch 135/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2533.0850 - val_loss: 2884.9563\n",
            "Epoch 136/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2525.3491 - val_loss: 2833.0232\n",
            "Epoch 137/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2511.0022 - val_loss: 2847.0620\n",
            "Epoch 138/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2507.8999 - val_loss: 2845.1057\n",
            "Epoch 139/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2489.3435 - val_loss: 2889.1746\n",
            "Epoch 140/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2500.8867 - val_loss: 2887.4216\n",
            "Epoch 141/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2492.4441 - val_loss: 2816.2068\n",
            "Epoch 142/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2482.9570 - val_loss: 2818.1223\n",
            "Epoch 143/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2472.3843 - val_loss: 2893.4573\n",
            "Epoch 144/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2469.6072 - val_loss: 2807.5479\n",
            "Epoch 145/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2458.6675 - val_loss: 2812.1379\n",
            "Epoch 146/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2457.4734 - val_loss: 2812.7942\n",
            "Epoch 147/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2471.8450 - val_loss: 2851.4214\n",
            "Epoch 148/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2453.8169 - val_loss: 2804.5134\n",
            "Epoch 149/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2457.2927 - val_loss: 2822.6892\n",
            "Epoch 150/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2434.4224 - val_loss: 2812.6238\n",
            "Epoch 151/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2422.4783 - val_loss: 2781.7415\n",
            "Epoch 152/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2425.3933 - val_loss: 2784.1855\n",
            "Epoch 153/200\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 2425.4473 - val_loss: 2788.3708\n",
            "Epoch 154/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2426.6538 - val_loss: 2776.6680\n",
            "Epoch 155/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2414.6360 - val_loss: 2816.2732\n",
            "Epoch 156/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2409.7178 - val_loss: 2762.6287\n",
            "Epoch 157/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2408.0295 - val_loss: 2780.1355\n",
            "Epoch 158/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2405.2329 - val_loss: 2749.8074\n",
            "Epoch 159/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2401.6892 - val_loss: 2724.6702\n",
            "Epoch 160/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2397.5637 - val_loss: 2739.9695\n",
            "Epoch 161/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2384.3201 - val_loss: 2725.3159\n",
            "Epoch 162/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2386.6992 - val_loss: 2743.2236\n",
            "Epoch 163/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2385.9062 - val_loss: 2774.2954\n",
            "Epoch 164/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2380.3352 - val_loss: 2721.3572\n",
            "Epoch 165/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2369.3643 - val_loss: 2755.5637\n",
            "Epoch 166/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2366.1025 - val_loss: 2680.1201\n",
            "Epoch 167/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2374.2556 - val_loss: 2691.9634\n",
            "Epoch 168/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2370.8997 - val_loss: 2723.7732\n",
            "Epoch 169/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2367.4521 - val_loss: 2696.1025\n",
            "Epoch 170/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2357.5522 - val_loss: 2679.1436\n",
            "Epoch 171/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2347.7974 - val_loss: 2672.1125\n",
            "Epoch 172/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2338.2349 - val_loss: 2686.6855\n",
            "Epoch 173/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2340.5103 - val_loss: 2747.5037\n",
            "Epoch 174/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2327.4878 - val_loss: 2650.6233\n",
            "Epoch 175/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2334.6484 - val_loss: 2734.5876\n",
            "Epoch 176/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2335.0852 - val_loss: 2670.3809\n",
            "Epoch 177/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2342.8853 - val_loss: 2662.8267\n",
            "Epoch 178/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2310.6506 - val_loss: 2656.5469\n",
            "Epoch 179/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2329.7532 - val_loss: 2674.1572\n",
            "Epoch 180/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2310.2295 - val_loss: 2696.8423\n",
            "Epoch 181/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2302.7781 - val_loss: 2659.9978\n",
            "Epoch 182/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2297.0010 - val_loss: 2618.9426\n",
            "Epoch 183/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2314.8503 - val_loss: 2601.6245\n",
            "Epoch 184/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2318.3401 - val_loss: 2623.4016\n",
            "Epoch 185/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2284.0649 - val_loss: 2592.5437\n",
            "Epoch 186/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2276.4773 - val_loss: 2652.5449\n",
            "Epoch 187/200\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 2285.8093 - val_loss: 2646.1736\n",
            "Epoch 188/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2281.0293 - val_loss: 2693.4402\n",
            "Epoch 189/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2278.5232 - val_loss: 2559.0332\n",
            "Epoch 190/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2272.0627 - val_loss: 2610.5198\n",
            "Epoch 191/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2264.6379 - val_loss: 2657.7188\n",
            "Epoch 192/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2281.4155 - val_loss: 2598.9766\n",
            "Epoch 193/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2250.3284 - val_loss: 2601.3779\n",
            "Epoch 194/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2239.3555 - val_loss: 2591.7144\n",
            "Epoch 195/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2238.0264 - val_loss: 2527.3362\n",
            "Epoch 196/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2242.7979 - val_loss: 2547.9292\n",
            "Epoch 197/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2246.5916 - val_loss: 2545.1421\n",
            "Epoch 198/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2234.4324 - val_loss: 2563.7192\n",
            "Epoch 199/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2221.7639 - val_loss: 2563.7551\n",
            "Epoch 200/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2230.4028 - val_loss: 2523.4026\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4a5534c450>"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lookup the documentation of Adam and find out what the first parameter is, what happens if you increase it by 10x?\n"
      ],
      "metadata": {
        "id": "I4EAOjW69_rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ins2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=.01), loss='mae')\n",
        "ins2.fit(X_train, y_train, validation_data=[X_test, y_test], epochs=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txTBUPk8cvtR",
        "outputId": "77dcb16e-d60d-49a7-bc12-382e274082bb"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "32/32 [==============================] - 1s 19ms/step - loss: 2764.7249 - val_loss: 2927.1997\n",
            "Epoch 2/200\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 2321.7981 - val_loss: 2553.9653\n",
            "Epoch 3/200\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2377.8611 - val_loss: 2784.2446\n",
            "Epoch 4/200\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 2243.2825 - val_loss: 2725.2463\n",
            "Epoch 5/200\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2265.5710 - val_loss: 2444.1848\n",
            "Epoch 6/200\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2276.7546 - val_loss: 3172.8018\n",
            "Epoch 7/200\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2301.3699 - val_loss: 2633.1650\n",
            "Epoch 8/200\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2216.9189 - val_loss: 2483.4604\n",
            "Epoch 9/200\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 2302.7544 - val_loss: 2734.5078\n",
            "Epoch 10/200\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 2102.6812 - val_loss: 2393.5757\n",
            "Epoch 11/200\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 2282.3772 - val_loss: 2639.7720\n",
            "Epoch 12/200\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2176.3591 - val_loss: 2442.0659\n",
            "Epoch 13/200\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2141.3877 - val_loss: 2520.3269\n",
            "Epoch 14/200\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2171.5352 - val_loss: 2389.5000\n",
            "Epoch 15/200\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2150.6763 - val_loss: 2367.2993\n",
            "Epoch 16/200\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 2157.1426 - val_loss: 2452.8455\n",
            "Epoch 17/200\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 2007.6862 - val_loss: 2408.9858\n",
            "Epoch 18/200\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 2043.3159 - val_loss: 2496.8462\n",
            "Epoch 19/200\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2037.8433 - val_loss: 2465.1382\n",
            "Epoch 20/200\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 2131.5273 - val_loss: 2431.7346\n",
            "Epoch 21/200\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 2033.5615 - val_loss: 2317.7156\n",
            "Epoch 22/200\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 2124.4685 - val_loss: 2904.4026\n",
            "Epoch 23/200\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2249.6890 - val_loss: 2266.6047\n",
            "Epoch 24/200\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2031.8575 - val_loss: 2664.6179\n",
            "Epoch 25/200\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 2056.0046 - val_loss: 2682.0969\n",
            "Epoch 26/200\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2026.3411 - val_loss: 2469.9365\n",
            "Epoch 27/200\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2018.9371 - val_loss: 2307.3525\n",
            "Epoch 28/200\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1971.4674 - val_loss: 2551.6174\n",
            "Epoch 29/200\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2036.6353 - val_loss: 2510.8489\n",
            "Epoch 30/200\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 2008.7914 - val_loss: 2305.6592\n",
            "Epoch 31/200\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 2113.3345 - val_loss: 2388.2532\n",
            "Epoch 32/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2006.2123 - val_loss: 2357.5693\n",
            "Epoch 33/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1986.4072 - val_loss: 2483.6807\n",
            "Epoch 34/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2197.5889 - val_loss: 2422.7988\n",
            "Epoch 35/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2069.1423 - val_loss: 2800.1699\n",
            "Epoch 36/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2136.0588 - val_loss: 2379.9673\n",
            "Epoch 37/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1993.2927 - val_loss: 2279.4397\n",
            "Epoch 38/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2095.3965 - val_loss: 2642.0891\n",
            "Epoch 39/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1884.0042 - val_loss: 2364.2461\n",
            "Epoch 40/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1981.7114 - val_loss: 2277.7385\n",
            "Epoch 41/200\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 2050.8484 - val_loss: 2311.4419\n",
            "Epoch 42/200\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1935.6400 - val_loss: 2254.7539\n",
            "Epoch 43/200\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1886.9128 - val_loss: 2298.5530\n",
            "Epoch 44/200\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 1890.4889 - val_loss: 2304.5747\n",
            "Epoch 45/200\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1891.3751 - val_loss: 2317.1436\n",
            "Epoch 46/200\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1972.7937 - val_loss: 2242.8770\n",
            "Epoch 47/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1909.7039 - val_loss: 2257.6035\n",
            "Epoch 48/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1964.1669 - val_loss: 2398.6016\n",
            "Epoch 49/200\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1948.1865 - val_loss: 2273.9121\n",
            "Epoch 50/200\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 1884.1586 - val_loss: 2253.8472\n",
            "Epoch 51/200\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 1860.0371 - val_loss: 2257.9380\n",
            "Epoch 52/200\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1856.9302 - val_loss: 2400.2209\n",
            "Epoch 53/200\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 1944.4631 - val_loss: 2294.6562\n",
            "Epoch 54/200\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1951.6819 - val_loss: 2406.8440\n",
            "Epoch 55/200\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 2053.6790 - val_loss: 2643.7029\n",
            "Epoch 56/200\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 1974.0063 - val_loss: 2235.9399\n",
            "Epoch 57/200\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 1865.5217 - val_loss: 2216.4138\n",
            "Epoch 58/200\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1871.3518 - val_loss: 2207.8005\n",
            "Epoch 59/200\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1898.5503 - val_loss: 2288.8713\n",
            "Epoch 60/200\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 1856.7456 - val_loss: 2387.2590\n",
            "Epoch 61/200\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 1923.9657 - val_loss: 2359.6226\n",
            "Epoch 62/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1915.3566 - val_loss: 2196.8635\n",
            "Epoch 63/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1858.6145 - val_loss: 2220.0012\n",
            "Epoch 64/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1819.0712 - val_loss: 2188.9404\n",
            "Epoch 65/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1848.5106 - val_loss: 2290.6553\n",
            "Epoch 66/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1886.2961 - val_loss: 2839.9062\n",
            "Epoch 67/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1999.7252 - val_loss: 2218.1062\n",
            "Epoch 68/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1777.1852 - val_loss: 2280.1423\n",
            "Epoch 69/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1791.8058 - val_loss: 2347.0308\n",
            "Epoch 70/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1845.5358 - val_loss: 2230.2991\n",
            "Epoch 71/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1952.5521 - val_loss: 2354.5603\n",
            "Epoch 72/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1822.3716 - val_loss: 2197.9033\n",
            "Epoch 73/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1800.8583 - val_loss: 2191.8804\n",
            "Epoch 74/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1848.5839 - val_loss: 2366.0684\n",
            "Epoch 75/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1870.0135 - val_loss: 2200.2463\n",
            "Epoch 76/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1811.3286 - val_loss: 2189.4656\n",
            "Epoch 77/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1793.6700 - val_loss: 2329.7136\n",
            "Epoch 78/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1992.8550 - val_loss: 2187.1841\n",
            "Epoch 79/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1855.4551 - val_loss: 2273.4783\n",
            "Epoch 80/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1814.8365 - val_loss: 2166.1174\n",
            "Epoch 81/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1748.8845 - val_loss: 2320.8269\n",
            "Epoch 82/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2018.5361 - val_loss: 2199.9390\n",
            "Epoch 83/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1816.0631 - val_loss: 2147.0903\n",
            "Epoch 84/200\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1821.5758 - val_loss: 2231.6255\n",
            "Epoch 85/200\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1833.1058 - val_loss: 2178.6777\n",
            "Epoch 86/200\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1928.2539 - val_loss: 2159.8914\n",
            "Epoch 87/200\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1807.7079 - val_loss: 2176.2876\n",
            "Epoch 88/200\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1822.6478 - val_loss: 2180.7380\n",
            "Epoch 89/200\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1870.6305 - val_loss: 2300.5154\n",
            "Epoch 90/200\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1775.6559 - val_loss: 2170.8296\n",
            "Epoch 91/200\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1791.6328 - val_loss: 2132.6309\n",
            "Epoch 92/200\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1791.1422 - val_loss: 2135.7434\n",
            "Epoch 93/200\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1731.3958 - val_loss: 2123.2356\n",
            "Epoch 94/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1737.2572 - val_loss: 2274.8618\n",
            "Epoch 95/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1723.9471 - val_loss: 2162.8430\n",
            "Epoch 96/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1911.9972 - val_loss: 2211.2202\n",
            "Epoch 97/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2068.9568 - val_loss: 2192.8943\n",
            "Epoch 98/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1877.5959 - val_loss: 2591.4543\n",
            "Epoch 99/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2064.8823 - val_loss: 2330.5774\n",
            "Epoch 100/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1808.4901 - val_loss: 2239.4143\n",
            "Epoch 101/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1824.9658 - val_loss: 2396.8914\n",
            "Epoch 102/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1815.4476 - val_loss: 2061.5918\n",
            "Epoch 103/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1723.8411 - val_loss: 2155.1438\n",
            "Epoch 104/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1725.1088 - val_loss: 2338.5803\n",
            "Epoch 105/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1784.2828 - val_loss: 2136.5122\n",
            "Epoch 106/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1709.9237 - val_loss: 2070.8137\n",
            "Epoch 107/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1716.4242 - val_loss: 2251.3418\n",
            "Epoch 108/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1694.7341 - val_loss: 2196.4136\n",
            "Epoch 109/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1763.4960 - val_loss: 2382.6230\n",
            "Epoch 110/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2164.6907 - val_loss: 2196.1382\n",
            "Epoch 111/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1698.7659 - val_loss: 2204.1333\n",
            "Epoch 112/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1728.8226 - val_loss: 2168.5486\n",
            "Epoch 113/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1694.7229 - val_loss: 2014.8625\n",
            "Epoch 114/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1742.0992 - val_loss: 2111.9109\n",
            "Epoch 115/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1742.2635 - val_loss: 2111.1047\n",
            "Epoch 116/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1679.2013 - val_loss: 2147.6033\n",
            "Epoch 117/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1711.3120 - val_loss: 2021.7034\n",
            "Epoch 118/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1783.9219 - val_loss: 2018.8075\n",
            "Epoch 119/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1729.0159 - val_loss: 2485.3352\n",
            "Epoch 120/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1685.3810 - val_loss: 2016.1332\n",
            "Epoch 121/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1691.7545 - val_loss: 2008.0865\n",
            "Epoch 122/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1791.0109 - val_loss: 2347.1694\n",
            "Epoch 123/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1754.6493 - val_loss: 2310.6033\n",
            "Epoch 124/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1753.6285 - val_loss: 2076.9434\n",
            "Epoch 125/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1780.1566 - val_loss: 2223.5100\n",
            "Epoch 126/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1972.8160 - val_loss: 2124.0796\n",
            "Epoch 127/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1695.9562 - val_loss: 2451.2449\n",
            "Epoch 128/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1893.4102 - val_loss: 2197.9717\n",
            "Epoch 129/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1813.0259 - val_loss: 2247.0728\n",
            "Epoch 130/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1715.1605 - val_loss: 2471.1826\n",
            "Epoch 131/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1758.2192 - val_loss: 2076.0693\n",
            "Epoch 132/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1739.9276 - val_loss: 2219.2078\n",
            "Epoch 133/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1741.8976 - val_loss: 2180.1287\n",
            "Epoch 134/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1842.2739 - val_loss: 2249.5520\n",
            "Epoch 135/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1735.9010 - val_loss: 2268.5115\n",
            "Epoch 136/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1794.7880 - val_loss: 2237.4680\n",
            "Epoch 137/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1818.9993 - val_loss: 2040.3987\n",
            "Epoch 138/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1706.3353 - val_loss: 2055.6650\n",
            "Epoch 139/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1704.4985 - val_loss: 2161.2356\n",
            "Epoch 140/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1745.2661 - val_loss: 2166.5305\n",
            "Epoch 141/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1709.9355 - val_loss: 2195.9631\n",
            "Epoch 142/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1743.8311 - val_loss: 2215.9885\n",
            "Epoch 143/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1865.1467 - val_loss: 2501.9592\n",
            "Epoch 144/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1826.2571 - val_loss: 2046.5646\n",
            "Epoch 145/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1747.5365 - val_loss: 2132.0991\n",
            "Epoch 146/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1745.7274 - val_loss: 2023.7037\n",
            "Epoch 147/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1797.9019 - val_loss: 2045.3041\n",
            "Epoch 148/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1705.4999 - val_loss: 2082.1785\n",
            "Epoch 149/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1602.1248 - val_loss: 1998.5612\n",
            "Epoch 150/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1701.3085 - val_loss: 2120.8005\n",
            "Epoch 151/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1622.8038 - val_loss: 2014.1553\n",
            "Epoch 152/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1698.3361 - val_loss: 2206.3118\n",
            "Epoch 153/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1659.3906 - val_loss: 2024.7084\n",
            "Epoch 154/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1644.6077 - val_loss: 2060.9487\n",
            "Epoch 155/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1646.7192 - val_loss: 2082.8481\n",
            "Epoch 156/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1706.1410 - val_loss: 2060.3137\n",
            "Epoch 157/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1713.3833 - val_loss: 2154.7449\n",
            "Epoch 158/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1719.7465 - val_loss: 2360.7917\n",
            "Epoch 159/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1842.1194 - val_loss: 2087.6338\n",
            "Epoch 160/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1705.6646 - val_loss: 1995.0991\n",
            "Epoch 161/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1632.9430 - val_loss: 2003.1726\n",
            "Epoch 162/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1644.9500 - val_loss: 2094.1787\n",
            "Epoch 163/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1628.3945 - val_loss: 1988.0808\n",
            "Epoch 164/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1623.5737 - val_loss: 2106.3149\n",
            "Epoch 165/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1688.7207 - val_loss: 2060.4565\n",
            "Epoch 166/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1668.5743 - val_loss: 2045.7280\n",
            "Epoch 167/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1741.2532 - val_loss: 2185.7432\n",
            "Epoch 168/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1644.7181 - val_loss: 1995.3115\n",
            "Epoch 169/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1763.2933 - val_loss: 1998.8308\n",
            "Epoch 170/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1643.8451 - val_loss: 2010.3357\n",
            "Epoch 171/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1690.6511 - val_loss: 2064.3550\n",
            "Epoch 172/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1711.6169 - val_loss: 2192.0579\n",
            "Epoch 173/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1774.4974 - val_loss: 2361.8611\n",
            "Epoch 174/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1640.1919 - val_loss: 1982.1713\n",
            "Epoch 175/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1690.0378 - val_loss: 2144.9326\n",
            "Epoch 176/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1761.0115 - val_loss: 1990.2921\n",
            "Epoch 177/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1778.5430 - val_loss: 2479.5312\n",
            "Epoch 178/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1808.3954 - val_loss: 2089.3938\n",
            "Epoch 179/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1702.5145 - val_loss: 1973.6360\n",
            "Epoch 180/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1806.4222 - val_loss: 1961.9138\n",
            "Epoch 181/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1673.6953 - val_loss: 2172.5898\n",
            "Epoch 182/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1683.2898 - val_loss: 2096.9258\n",
            "Epoch 183/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1633.3422 - val_loss: 2081.5916\n",
            "Epoch 184/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1593.6808 - val_loss: 2202.1274\n",
            "Epoch 185/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1837.7982 - val_loss: 2370.6738\n",
            "Epoch 186/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1667.2393 - val_loss: 2014.0125\n",
            "Epoch 187/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1716.5974 - val_loss: 2196.5413\n",
            "Epoch 188/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1734.6539 - val_loss: 2148.0251\n",
            "Epoch 189/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1620.8519 - val_loss: 2182.3745\n",
            "Epoch 190/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1711.5481 - val_loss: 1994.7472\n",
            "Epoch 191/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1684.0928 - val_loss: 2212.1499\n",
            "Epoch 192/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1770.1443 - val_loss: 2274.9658\n",
            "Epoch 193/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1710.7391 - val_loss: 2069.8542\n",
            "Epoch 194/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1654.8218 - val_loss: 1939.3693\n",
            "Epoch 195/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1551.1195 - val_loss: 1966.9054\n",
            "Epoch 196/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1586.6688 - val_loss: 2028.2739\n",
            "Epoch 197/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1544.5312 - val_loss: 1948.9385\n",
            "Epoch 198/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1604.8290 - val_loss: 2047.6711\n",
            "Epoch 199/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1730.7278 - val_loss: 2671.2979\n",
            "Epoch 200/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1815.4257 - val_loss: 1875.6918\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4a55176850>"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What happens if you train for longer (say 300 epochs instead of 200)?\n",
        "ins2.fit(X_train, y_train, validation_data=[X_test, y_test], epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6DPUuRUcwgF",
        "outputId": "40f1a560-9bd1-4524-a8d9-fb99e5558483"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 1585.3306 - val_loss: 2059.9543\n",
            "Epoch 2/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1637.5115 - val_loss: 1915.4528\n",
            "Epoch 3/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1613.0405 - val_loss: 2026.5280\n",
            "Epoch 4/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1657.9374 - val_loss: 2033.1953\n",
            "Epoch 5/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1574.2886 - val_loss: 1988.8153\n",
            "Epoch 6/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1606.2623 - val_loss: 1994.4926\n",
            "Epoch 7/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1570.5277 - val_loss: 1991.3673\n",
            "Epoch 8/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1680.0189 - val_loss: 1880.2126\n",
            "Epoch 9/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1598.4630 - val_loss: 1979.2192\n",
            "Epoch 10/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1602.4336 - val_loss: 1985.4637\n",
            "Epoch 11/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1591.2725 - val_loss: 1904.1367\n",
            "Epoch 12/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1533.3685 - val_loss: 1922.9565\n",
            "Epoch 13/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1587.0717 - val_loss: 1906.8079\n",
            "Epoch 14/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1564.4219 - val_loss: 1876.9569\n",
            "Epoch 15/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1639.1936 - val_loss: 1854.8104\n",
            "Epoch 16/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1603.7131 - val_loss: 1907.2458\n",
            "Epoch 17/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1502.0906 - val_loss: 1844.0942\n",
            "Epoch 18/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1493.8093 - val_loss: 1920.3774\n",
            "Epoch 19/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1668.4078 - val_loss: 2000.7161\n",
            "Epoch 20/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1582.1052 - val_loss: 1935.8796\n",
            "Epoch 21/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1521.6769 - val_loss: 2038.3052\n",
            "Epoch 22/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1690.7666 - val_loss: 1948.6501\n",
            "Epoch 23/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1536.8784 - val_loss: 1872.5261\n",
            "Epoch 24/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1522.3375 - val_loss: 1974.3086\n",
            "Epoch 25/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1619.6954 - val_loss: 2096.7427\n",
            "Epoch 26/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1570.0962 - val_loss: 1991.2565\n",
            "Epoch 27/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1590.3492 - val_loss: 1961.7280\n",
            "Epoch 28/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1585.4187 - val_loss: 1975.9634\n",
            "Epoch 29/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1552.3998 - val_loss: 1840.6158\n",
            "Epoch 30/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1558.0284 - val_loss: 1941.2416\n",
            "Epoch 31/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1861.1356 - val_loss: 1977.6826\n",
            "Epoch 32/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1612.1179 - val_loss: 2015.8110\n",
            "Epoch 33/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1643.8820 - val_loss: 2109.7227\n",
            "Epoch 34/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1651.8644 - val_loss: 2357.8547\n",
            "Epoch 35/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1749.7338 - val_loss: 2013.8276\n",
            "Epoch 36/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1594.4545 - val_loss: 2146.4561\n",
            "Epoch 37/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1654.3928 - val_loss: 1972.2673\n",
            "Epoch 38/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1542.9478 - val_loss: 2038.8892\n",
            "Epoch 39/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1515.3789 - val_loss: 1917.9026\n",
            "Epoch 40/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1681.9335 - val_loss: 1848.7858\n",
            "Epoch 41/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1691.1318 - val_loss: 2433.8103\n",
            "Epoch 42/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1591.1002 - val_loss: 2010.4608\n",
            "Epoch 43/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1553.6609 - val_loss: 1897.4457\n",
            "Epoch 44/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1520.6204 - val_loss: 1856.5114\n",
            "Epoch 45/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1584.9857 - val_loss: 2025.6544\n",
            "Epoch 46/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1527.0516 - val_loss: 2026.5985\n",
            "Epoch 47/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1621.0050 - val_loss: 2164.2161\n",
            "Epoch 48/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1666.5135 - val_loss: 2324.1948\n",
            "Epoch 49/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1714.6002 - val_loss: 1908.9491\n",
            "Epoch 50/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1506.5216 - val_loss: 1813.0861\n",
            "Epoch 51/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1582.2140 - val_loss: 2186.0295\n",
            "Epoch 52/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1577.0791 - val_loss: 1961.5657\n",
            "Epoch 53/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1562.2504 - val_loss: 1961.7079\n",
            "Epoch 54/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1500.1437 - val_loss: 1949.8262\n",
            "Epoch 55/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1595.4185 - val_loss: 2003.0961\n",
            "Epoch 56/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1536.4269 - val_loss: 1977.0905\n",
            "Epoch 57/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1492.6344 - val_loss: 1981.8387\n",
            "Epoch 58/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1575.5707 - val_loss: 1992.6741\n",
            "Epoch 59/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1653.9866 - val_loss: 2026.5457\n",
            "Epoch 60/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1553.1187 - val_loss: 1888.9318\n",
            "Epoch 61/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1583.8997 - val_loss: 2059.5081\n",
            "Epoch 62/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1666.1891 - val_loss: 2214.5396\n",
            "Epoch 63/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1682.5426 - val_loss: 1902.1554\n",
            "Epoch 64/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1563.5128 - val_loss: 2059.3167\n",
            "Epoch 65/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1589.5948 - val_loss: 1884.8923\n",
            "Epoch 66/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1519.5190 - val_loss: 2166.6702\n",
            "Epoch 67/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1637.8285 - val_loss: 1957.3804\n",
            "Epoch 68/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1597.9556 - val_loss: 2245.2671\n",
            "Epoch 69/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1614.0667 - val_loss: 1812.5457\n",
            "Epoch 70/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1519.6328 - val_loss: 1915.6696\n",
            "Epoch 71/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1579.8975 - val_loss: 2203.1863\n",
            "Epoch 72/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1519.1824 - val_loss: 1795.8500\n",
            "Epoch 73/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1483.0740 - val_loss: 1868.1454\n",
            "Epoch 74/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1518.4595 - val_loss: 1836.4668\n",
            "Epoch 75/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1503.4536 - val_loss: 1807.3188\n",
            "Epoch 76/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1505.2656 - val_loss: 1964.5194\n",
            "Epoch 77/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1552.1650 - val_loss: 2064.0535\n",
            "Epoch 78/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1544.3400 - val_loss: 1846.9279\n",
            "Epoch 79/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1491.3422 - val_loss: 1948.4043\n",
            "Epoch 80/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1593.8472 - val_loss: 1972.9167\n",
            "Epoch 81/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1539.7551 - val_loss: 1865.4731\n",
            "Epoch 82/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1547.6003 - val_loss: 1973.3696\n",
            "Epoch 83/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1613.5880 - val_loss: 2067.5356\n",
            "Epoch 84/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1619.4037 - val_loss: 1905.6819\n",
            "Epoch 85/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1613.2057 - val_loss: 1826.2356\n",
            "Epoch 86/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1495.8740 - val_loss: 1888.7803\n",
            "Epoch 87/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1526.0294 - val_loss: 2038.9912\n",
            "Epoch 88/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1577.0308 - val_loss: 1883.0209\n",
            "Epoch 89/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1602.4772 - val_loss: 1974.5168\n",
            "Epoch 90/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1573.1525 - val_loss: 1859.2140\n",
            "Epoch 91/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1504.6947 - val_loss: 1840.7930\n",
            "Epoch 92/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1507.5579 - val_loss: 1845.1528\n",
            "Epoch 93/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1594.2242 - val_loss: 2070.9751\n",
            "Epoch 94/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1562.8893 - val_loss: 1852.4713\n",
            "Epoch 95/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1538.9653 - val_loss: 1823.1802\n",
            "Epoch 96/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1692.5632 - val_loss: 2163.8938\n",
            "Epoch 97/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1792.3960 - val_loss: 2144.0110\n",
            "Epoch 98/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1543.8032 - val_loss: 2027.3884\n",
            "Epoch 99/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1570.0693 - val_loss: 1922.0160\n",
            "Epoch 100/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1511.1301 - val_loss: 1950.9021\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4a55023510>"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import the Boston pricing dataset from TensorFlow tf.keras.datasets and model it.\n"
      ],
      "metadata": {
        "id": "jv8iNnle94k7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = \\\n",
        "\\\n",
        " tf.keras.datasets.boston_housing.load_data(\n",
        "    path='boston_housing.npz', test_split=0.2, seed=113\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDzhXBjRcyHb",
        "outputId": "8dad3db0-8830-4398-a3cc-663b0b31ee88"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57344/57026 [==============================] - 0s 0us/step\n",
            "65536/57026 [==================================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjCMuWUm7L1J",
        "outputId": "c1375d9b-acde-4c1e-ac29-4e5ab6fc80e6"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(404, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bos = tf.keras.Sequential()\n",
        "bos.add(tf.keras.layers.Input([X_train.shape[1]]))\n",
        "bos.add(tf.keras.layers.Normalization())\n",
        "bos.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "bos.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "bos.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "bos.compile(optimizer='Adam', loss='mse', metrics='mae')"
      ],
      "metadata": {
        "id": "e__-n3957S7o"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = bos.fit(X_train, y_train, validation_data=[X_test, y_test], epochs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9ERYvTL7tRt",
        "outputId": "78d6d9af-10a5-440c-9216-6d2b6a4f4aba"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "13/13 [==============================] - 1s 21ms/step - loss: 5187.7764 - mae: 64.5090 - val_loss: 826.0698 - val_mae: 26.4149\n",
            "Epoch 2/20\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 866.2758 - mae: 22.6933 - val_loss: 766.4521 - val_mae: 21.0634\n",
            "Epoch 3/20\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 308.1059 - mae: 12.2945 - val_loss: 103.4961 - val_mae: 8.1616\n",
            "Epoch 4/20\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 133.5363 - mae: 9.3606 - val_loss: 113.6791 - val_mae: 8.3269\n",
            "Epoch 5/20\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 101.1086 - mae: 7.3868 - val_loss: 105.3358 - val_mae: 7.6446\n",
            "Epoch 6/20\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 84.8667 - mae: 6.6122 - val_loss: 84.2040 - val_mae: 7.0857\n",
            "Epoch 7/20\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 75.8353 - mae: 6.4311 - val_loss: 79.3787 - val_mae: 6.8857\n",
            "Epoch 8/20\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 71.4750 - mae: 6.1798 - val_loss: 75.9297 - val_mae: 6.6679\n",
            "Epoch 9/20\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 67.6399 - mae: 5.9259 - val_loss: 73.9395 - val_mae: 6.4708\n",
            "Epoch 10/20\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 64.9434 - mae: 5.7612 - val_loss: 71.6007 - val_mae: 6.3159\n",
            "Epoch 11/20\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 62.8182 - mae: 5.6400 - val_loss: 69.3703 - val_mae: 6.1938\n",
            "Epoch 12/20\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 61.3360 - mae: 5.5709 - val_loss: 68.4402 - val_mae: 6.0997\n",
            "Epoch 13/20\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 60.2180 - mae: 5.3858 - val_loss: 65.9877 - val_mae: 6.0065\n",
            "Epoch 14/20\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 58.8203 - mae: 5.4737 - val_loss: 65.2831 - val_mae: 5.9394\n",
            "Epoch 15/20\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 58.0634 - mae: 5.2660 - val_loss: 64.4164 - val_mae: 5.8705\n",
            "Epoch 16/20\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 57.0048 - mae: 5.3431 - val_loss: 62.9917 - val_mae: 5.7940\n",
            "Epoch 17/20\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 56.2445 - mae: 5.2375 - val_loss: 62.7135 - val_mae: 5.7426\n",
            "Epoch 18/20\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 55.6047 - mae: 5.2164 - val_loss: 61.8296 - val_mae: 5.6868\n",
            "Epoch 19/20\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 55.1772 - mae: 5.1374 - val_loss: 60.5190 - val_mae: 5.6512\n",
            "Epoch 20/20\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 54.7647 - mae: 5.3009 - val_loss: 61.0446 - val_mae: 5.6061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-TIx88p8OFL",
        "outputId": "8a4e4b0c-726d-45af-fd92-383d617e0bb4"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [5187.7763671875,\n",
              "  866.2758178710938,\n",
              "  308.1059265136719,\n",
              "  133.53631591796875,\n",
              "  101.10860443115234,\n",
              "  84.8666763305664,\n",
              "  75.83527374267578,\n",
              "  71.47501373291016,\n",
              "  67.6399154663086,\n",
              "  64.94342803955078,\n",
              "  62.81817626953125,\n",
              "  61.33600616455078,\n",
              "  60.218040466308594,\n",
              "  58.820255279541016,\n",
              "  58.063419342041016,\n",
              "  57.004844665527344,\n",
              "  56.24446487426758,\n",
              "  55.60466766357422,\n",
              "  55.17719268798828,\n",
              "  54.76469802856445],\n",
              " 'mae': [64.50896453857422,\n",
              "  22.693267822265625,\n",
              "  12.294504165649414,\n",
              "  9.360591888427734,\n",
              "  7.386808395385742,\n",
              "  6.612185001373291,\n",
              "  6.431079864501953,\n",
              "  6.179832935333252,\n",
              "  5.925902843475342,\n",
              "  5.761238098144531,\n",
              "  5.639997959136963,\n",
              "  5.570934295654297,\n",
              "  5.385799407958984,\n",
              "  5.473673343658447,\n",
              "  5.265995979309082,\n",
              "  5.343149185180664,\n",
              "  5.237521648406982,\n",
              "  5.216403007507324,\n",
              "  5.137425422668457,\n",
              "  5.300925254821777],\n",
              " 'val_loss': [826.0697631835938,\n",
              "  766.4521484375,\n",
              "  103.4960708618164,\n",
              "  113.67911529541016,\n",
              "  105.33583068847656,\n",
              "  84.20398712158203,\n",
              "  79.37870025634766,\n",
              "  75.9297103881836,\n",
              "  73.93952941894531,\n",
              "  71.60066986083984,\n",
              "  69.37031555175781,\n",
              "  68.44019317626953,\n",
              "  65.98771667480469,\n",
              "  65.2831039428711,\n",
              "  64.41639709472656,\n",
              "  62.991729736328125,\n",
              "  62.71345138549805,\n",
              "  61.82960891723633,\n",
              "  60.51897430419922,\n",
              "  61.0445671081543],\n",
              " 'val_mae': [26.41487693786621,\n",
              "  21.063400268554688,\n",
              "  8.161636352539062,\n",
              "  8.326861381530762,\n",
              "  7.644604206085205,\n",
              "  7.085723400115967,\n",
              "  6.885729789733887,\n",
              "  6.667946815490723,\n",
              "  6.470755100250244,\n",
              "  6.31589937210083,\n",
              "  6.193788528442383,\n",
              "  6.099661350250244,\n",
              "  6.00654935836792,\n",
              "  5.939400672912598,\n",
              "  5.870491981506348,\n",
              "  5.793957233428955,\n",
              "  5.742561340332031,\n",
              "  5.686778545379639,\n",
              "  5.651156425476074,\n",
              "  5.606105804443359]}"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "hq7FyfAK9F9k"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history, metric=None):\n",
        "  ans = history.history\n",
        "\n",
        "  pd.Series(ans['loss']).plot()\n",
        "  pd.Series(ans[f'val_loss']).plot()\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.show()\n",
        "\n",
        "  if metric is not None:\n",
        "    pd.Series(ans[metric]).plot()\n",
        "    pd.Series(ans[f'val_{metric}']).plot()\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel(metric)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "R1UEevEU78my"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(history, metric='mae')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "Ks7HJFuW9Ckq",
        "outputId": "06ece57d-2351-4820-ac5b-b023c219737c"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Z1nJkSYq9ECF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}